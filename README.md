# Projet ATLAS

### A.T.L.A.S. ‚Äî Analyse Textuelle et Localisation des Annonces Sp√©cialis√©es

<p align="center">
  <img src="images/Logo.png" alt="Logo ATLAS" width="250">
</p>

ATLAS est un projet de Text Mining / NLP appliqu√© aux offres d‚Äôemploi, avec une dimension g√©ographique, une couche de web scraping, une base de donn√©es mod√©lis√©e en entrep√¥t, et une application interactive pour explorer et visualiser les analyses.

# Objectifs du projet

Le projet vise √† :

- Collecter automatiquement des offres d‚Äôemploi (web scraping / API).
- Construire un corpus annot√© et structur√©, centr√© sur un domaine (IA, data, ML‚Ä¶).
- Mod√©liser une base de donn√©es entrep√¥t (table de faits + dimensions).
- D√©velopper une application interactive (Dash, Bokeh, Streamlit‚Ä¶) permettant :

  - L‚Äôexploration du corpus
  - La recherche d‚Äôannonces
  - L‚Äôanalyse de texte (NLP)
  - La visualisation cartographique
  - La possibilit√© d‚Äôajouter dynamiquement de nouvelles offres

- D√©ployer l‚Äôensemble avec Docker

## üìπ Vid√©o de d√©monstration

> üé• **Installation et utilisation** : [Lien vers la vid√©o](#)  
> _(Remplacez le # par l'URL de votre vid√©o)_

## üöÄ Installation et lancement

### Option 1 : Lancement avec Docker (recommand√©)

La m√©thode la plus simple pour d√©marrer l'ensemble du projet :

```bash
# Cloner le projet
git clone <url-du-repo>
cd Projet-ATLAS

# Lancer tous les services (API + Streamlit + Base de donn√©es)
docker-compose up --build -d

# V√©rifier que les containers sont d√©marr√©s
docker-compose ps

# Acc√©der √† l'application
# - Streamlit : http://localhost:8501
# - API FastAPI : http://localhost:8000
# - Documentation API : http://localhost:8000/docs
```

Pour arr√™ter les services :

```bash
docker-compose down
```

### Option 2 : Lancement en local (d√©veloppement)

Pour ex√©cuter l'application en mode d√©veloppement sans Docker :

#### Pr√©requis

- Python 3.11+
- PostgreSQL (ou acc√®s √† Supabase)
- Mod√®les NLP pr√©-entra√Æn√©s

#### Installation

```bash
# 1. Cr√©er un environnement virtuel
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
.\venv\Scripts\activate  # Windows

# 2. Installer les d√©pendances API
cd api
pip install -r requirements.txt
python -m spacy download fr_core_news_md

# 3. Installer les d√©pendances Streamlit
cd ../streamlit_app
pip install -r requirements.txt

# 4. Installer les d√©pendances collectors (optionnel)
cd ../collectors
pip install -r requirements.txt
```

#### Lancement de l'API

```bash
# Depuis le dossier racine du projet
cd api
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

L'API sera accessible sur : http://localhost:8000

#### Lancement de Streamlit

```bash
# Dans un autre terminal, depuis le dossier racine
cd streamlit_app
streamlit run app.py
```

L'interface Streamlit sera accessible sur : http://localhost:8501

#### Configuration

Assurez-vous de configurer les variables d'environnement n√©cessaires :

- `DATABASE_URL` : URL de connexion PostgreSQL
- `API_URL` : URL de l'API (par d√©faut : http://localhost:8000)
- `RAPIDAPI_KEY`: Cl√© API pour Glassdoor

### üìä G√©n√©ration des mod√®les NLP (optionnel)

Si vous souhaitez r√©g√©n√©rer les mod√®les de topic modeling :

```bash
cd NLP/scripts
python topic_modeling_full.py
```
