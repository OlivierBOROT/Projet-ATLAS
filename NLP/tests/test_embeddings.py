"""
Script de test des embeddings
Calcule et compare les embeddings des offres d'exemple
"""

import json
import sys
import os
from pathlib import Path
import numpy as np

# Ajouter le chemin des modules NLP
sys.path.insert(0, str(Path(__file__).parent.parent / "modules"))

from text_cleaner import TextCleaner
from embedding_generator import EmbeddingGenerator


def main():
    print("=" * 80)
    print("üß™ TEST DES EMBEDDINGS")
    print("=" * 80)
    print()

    # Charger les offres d'exemple
    json_path = Path(__file__).parent / "example_emb_offers.json"

    print(f"üìÇ Chargement des offres depuis : {json_path.name}")
    with open(json_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    offers = data["offers"]
    print(f"‚úÖ {len(offers)} offres charg√©es\n")

    # Initialiser les modules
    print("‚è≥ Initialisation des modules...")
    cleaner = TextCleaner()
    embedding_gen = EmbeddingGenerator()

    # Afficher les infos du mod√®le
    model_info = embedding_gen.get_model_info()
    print(f"‚úÖ Mod√®le : {model_info['model_name']}")
    print(f"   Dimensions : {model_info['embedding_dimension']}")
    print()

    # Traiter chaque offre
    results = []

    print("üî¨ TRAITEMENT DES OFFRES")
    print("-" * 80)

    for offer in offers:
        offer_id = offer["offer_id"]
        text = offer["text"]

        print(f"\nüìÑ Offre #{offer_id}")
        print(f"   Texte original : {len(text)} caract√®res")

        # Nettoyer et lemmatiser
        cleaned = cleaner.clean_text(text)
        lemmas = cleaner.lemmatize(cleaned)
        text_cleaned = " ".join(lemmas)

        print(f"   Texte nettoy√© : {len(text_cleaned)} caract√®res")
        print(f"   Extrait : {text_cleaned[:100]}...")

        # Calculer l'embedding
        embedding = embedding_gen.generate(text_cleaned)

        print(f"   Embedding : vecteur de {len(embedding)} dimensions")
        print(f"   Norme L2 : {np.linalg.norm(embedding):.4f}")
        print(f"   Min/Max : [{embedding.min():.4f}, {embedding.max():.4f}]")

        results.append(
            {
                "offer_id": offer_id,
                "text": text,
                "text_cleaned": text_cleaned,
                "embedding": embedding,
                "text_length": len(text),
                "cleaned_length": len(text_cleaned),
            }
        )

    # Analyse des similarit√©s
    print("\n" + "=" * 80)
    print("üìä ANALYSE DES SIMILARIT√âS")
    print("=" * 80)
    print()

    # Matrice de similarit√©
    n = len(results)
    similarity_matrix = np.zeros((n, n))

    for i in range(n):
        for j in range(n):
            similarity_matrix[i, j] = embedding_gen.cosine_similarity(
                results[i]["embedding"], results[j]["embedding"]
            )

    print("üìê Matrice de similarit√© cosinus :")
    print()

    # Header
    header = "        " + "".join([f"  Off#{r['offer_id']}" for r in results])
    print(header)
    print("-" * len(header))

    # Lignes
    for i, result in enumerate(results):
        row = f"Off#{result['offer_id']:<3} |"
        for j in range(n):
            sim = similarity_matrix[i, j]
            if i == j:
                row += f"  1.0000"
            else:
                row += f"  {sim:.4f}"
        print(row)

    # Statistiques
    print("\nüìä STATISTIQUES :")
    print()

    # Similarit√©s entre offres diff√©rentes
    off_diagonal = []
    for i in range(n):
        for j in range(i + 1, n):
            off_diagonal.append(similarity_matrix[i, j])

    if off_diagonal:
        print(
            f"   Similarit√© moyenne (offres diff√©rentes) : {np.mean(off_diagonal):.4f}"
        )
        print(f"   Similarit√© min : {np.min(off_diagonal):.4f}")
        print(f"   Similarit√© max : {np.max(off_diagonal):.4f}")
        print(f"   √âcart-type : {np.std(off_diagonal):.4f}")

    # V√©rification des textes identiques
    print("\nüîç V√âRIFICATION DES TEXTES :")
    print()

    unique_texts = {}
    for result in results:
        text_hash = hash(result["text"])
        if text_hash not in unique_texts:
            unique_texts[text_hash] = [result["offer_id"]]
        else:
            unique_texts[text_hash].append(result["offer_id"])

    if len(unique_texts) == 1:
        print("   ‚ö†Ô∏è  TOUTES les offres ont le M√äME texte !")
        print(
            f"   Offres identiques : {', '.join([f'#{oid}' for oid in list(unique_texts.values())[0]])}"
        )
    else:
        print(f"   ‚úÖ {len(unique_texts)} textes uniques d√©tect√©s")
        for i, (text_hash, offer_ids) in enumerate(unique_texts.items(), 1):
            if len(offer_ids) > 1:
                print(
                    f"   Groupe {i} : Offres {', '.join([f'#{oid}' for oid in offer_ids])} (texte identique)"
                )

    # V√©rification de la coh√©rence des embeddings
    print("\n‚úÖ VALIDATION :")
    print()

    # Si textes identiques, embeddings doivent √™tre presque identiques
    if len(unique_texts) == 1:
        max_diff = 0
        for i in range(n):
            for j in range(i + 1, n):
                diff = np.abs(results[i]["embedding"] - results[j]["embedding"]).max()
                max_diff = max(max_diff, diff)

        print(f"   Diff√©rence max entre embeddings : {max_diff:.6f}")

        if max_diff < 1e-6:
            print("   ‚úÖ Embeddings identiques (diff√©rence n√©gligeable)")
        elif max_diff < 1e-3:
            print("   ‚úÖ Embeddings tr√®s similaires (diff√©rence acceptable)")
        else:
            print(
                "   ‚ö†Ô∏è  Embeddings diff√®rent plus que pr√©vu pour des textes identiques"
            )

    print("\n" + "=" * 80)
    print("‚úÖ TEST TERMIN√â")
    print("=" * 80)


if __name__ == "__main__":
    main()
